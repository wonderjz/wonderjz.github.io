---
title: AIPW
date: 2025-07-03 20:00:00 +/-TTTT +0800
categories: [Causal]
tags: [causalinf]     # TAG names should always be lowercase
math: true
---

Conditional Independence Assumption (CIA)
It requires that, given the covariate $X$, the treatment assignment $T$ and the potential outcome $Y$ are conditionally independent. That is, if all confounding variables are included in $X$, then controlling for $X$, the treatment assignment is close to random, and has no association with the potential outcome. Mathematically, it can be expressed as:
$(Y(0), Y(1)) \perp T \mid X$
where $Y(0)$ and $Y(1)$ are potential outcomes under control and treatment conditions, respectively, and $T$ is the treatment variable.

Overlap Assumption
Also known as the common-support assumption, it requires that for every feasible value of the covariate $X = x$, there exist observed individuals in both the treatment group and the control group. For discrete covariates, each value must appear in both groups; for continuous covariates, their distributions must overlap. Formally, this is:$0 < P(T = 1 \mid X = x) < 1$
for all feasible $x$. This ensures that there are comparable samples (identical or similar in covariates) for matching between the treatment and control groups.



Causal Estimation Bias Caused by Non - overlap: When the overlap assumption is not satisfied, that is, there is no overlap in the covariate distribution between the treatment group and the control group, it will lead to the inability to find suitable matching objects. At this time, the conditional independence assumption is difficult to hold in practice. Because if there is no overlap in covariates, it means that there are fundamental differences in the characteristics of the treatment group and the control group, and it is impossible to make the treatment assignment approximately random by controlling covariates, which will lead to the deviation of causal effect estimation. For example, if the treatment group is all young people and the control group is all elderly people, and age is a key covariate, then the treatment effect estimated without considering this non - overlap will be seriously biased, and the conditional independence assumption is no longer valid.

Conflicts: The more covariates we have, the easier the CI assumption is satisfied. Certain specific values of covariates may not be observed in some treatment groups, which means the violation of the overlap assumption.

We need to select variables that matter to both control and treatment.

Do selection!

The inference should be robust to model-selection mistakes. We admit that we made the model selection and that we may select the wrong variables. =â‡’ Neyman orthognality

$$
\begin{cases}
y = g(\tau, \mathbf{x}) + u, & \mathbb{E}[u \mid \mathbf{x}, \tau] = 0 \\
\tau = m(\mathbf{x}) + v, & \mathbb{E}[v \mid \mathbf{x}, \tau] = 0
\end{cases}
$$  

where  

- $ y $ is the observed outcome,  
- $ \tau $ is the treatment status (1 = treated, 0 = untreated),  
- $ g(1, \mathbf{x}) \equiv \mathbb{E}[Y(1) \mid \mathbf{x}] $ and $ g(0, \mathbf{x}) \equiv \mathbb{E}[Y(0) \mid \mathbf{x}] $,  
- $ m(\mathbf{x}) \equiv \Pr[\tau = 1 \mid \mathbf{x}] $ (propensity score).  


$$
\text{ATE} = \mathbb{E}[Y(1) - Y(0)] = \mathbb{E}\left[ \mathbb{E}[Y(1) \mid \mathbf{x}] - \mathbb{E}[Y(0) \mid \mathbf{x}] \right] = \mathbb{E}\left[ g(1, \mathbf{x}) - g(0, \mathbf{x}) \right]
$$  

$$
\text{ATE} = \mathbb{E}\left[ Y(1, \mathbf{x})_{\text{AIPW}} - Y(0, \mathbf{x})_{\text{AIPW}} \right]
$$  


where  
- $Y(1, \mathbf{x})_{\text{AIPW}} = g(1, \mathbf{x}) + \dfrac{\tau \left( y - g(1, \mathbf{x}) \right)}{m(\mathbf{x})}$  
- $Y(0, \mathbf{x})_{\text{AIPW}} = g(0, \mathbf{x}) + \dfrac{(1 - \tau) \left( y - g(0, \mathbf{x}) \right)}{1 - m(\mathbf{x})}$  


Notice that:  

$$
\text{ATE} = \mathbb{E}\left[ g(1, \mathbf{x}) - g(0, \mathbf{x}) \right]
$$  


The red terms are *Augmented terms* using the **Inverse Probability Weighting**; thus AIPW was born.


Double machine learning means cross-fitting + resampling
(Get the estimate of parameters in the first part and the estimate the ATE(target function) in the second part, and then cross-fit. Finally, averging the estimated ATE)
